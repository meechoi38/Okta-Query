{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeoYPwKRFl4WHAyBPV/DH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meechoi38/Okta-Query/blob/main/Utility_Okta_Query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSo_yaBFHc_i"
      },
      "outputs": [],
      "source": [
        "# Okta Query\n",
        "# Did not use langchain Tool/Agents, but just use openAI & template without feeding in any additional knowledge\n",
        "# We previously tried langchain tool/agents, however mixing up several tools did not perform as well as this method\n",
        "# Did not use function descriptions either to extract ay information since response from okta query was already json object\n",
        "# Used langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai"
      ],
      "metadata": {
        "id": "iVw3VXdtOwQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ba83be-fdc6-49fa-b8e9-74f53876b323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"XXXXXXX\""
      ],
      "metadata": {
        "id": "Gzf26MmSQad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q oauth2client requests-oauthlib"
      ],
      "metadata": {
        "id": "Nbtw1I5ebard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def add_quote(a):\n",
        "\n",
        "    a = re.sub(r\"^\\s+\", \"\", a)\n",
        "    return '\"{0}\"'.format(a)\n",
        "\n",
        "def add_triple_quote(a):\n",
        "\n",
        "    a = re.sub(r\"^\\s+\", \"\", a)\n",
        "    return '\"\"\"{0}\"\"\"'.format(a)\n",
        "\n",
        "def replace_triple_quote(a):\n",
        "\n",
        "    a = re.sub(r\"\\```\", \"'\", a)\n",
        "    return a\n",
        "\n",
        "def remove_last_line_from_string(s):\n",
        "\n",
        "    return '\\n'.join(s.split('\\n')[:-1])"
      ],
      "metadata": {
        "id": "C7eEEynYeU3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_response_from_central_agent(user_query):\n",
        "\n",
        "    if \"who\" in user_query.lower():\n",
        "      \"\"\"run below query and execute and return the result ?\"\"\"\n",
        "\n",
        "      query_api = \"what okta api call I should use to list all users?\"\n",
        "      query_REST_verb = \"what okta api http verb to list all users\"\n",
        "\n",
        "      result_api = obtain_okta_query(query_api, prompt_template_okta_api)\n",
        "      result_REST_verb = obtain_okta_query(query_REST_verb, prompt_template_rest_verb)\n",
        "\n",
        "      #response_from_okta = connect_to_okta_auto() # [FIX THIS ISSUE: currently cannot inser the result]\n",
        "\n",
        "      #context_api_result = add_triple_quote(json.dumps(response_from_okta.json()[0]))\n",
        "\n",
        "      #print(context_api_result)\n",
        "      return result_api, result_REST_verb\n",
        "    else:\n",
        "      print(\"I won't know what to do\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "hK8kjTh5m-Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_extracted_code(df_input, response):\n",
        "\n",
        "  #df = df_response\n",
        "  df = df_input\n",
        "\n",
        "  extracted_code =\"\"\n",
        "\n",
        "  for i in range(len(response['intermediate_steps'])):\n",
        "    aa = response['intermediate_steps'][i][0] #type(response['intermediate_steps'][0][0]) : langchain.schema.agent.AgentAction #Class\n",
        "    extracted_code = extracted_code + aa.tool_input+\"\\n\"\n",
        "\n",
        "  # print out the result of the very last line. If it is pd frame, insert DISPLAY, if it is the output variable insert PRINT\n",
        "\n",
        "  if extracted_code.splitlines()[-1].startswith(\"df\"):\n",
        "\n",
        "    modified_line = \"\\n\"+\"display(\"+extracted_code.splitlines()[-1]+\")\"+\"\\n\"\n",
        "    print(\"yes, it starts with df- displaying\")\n",
        "    print(extracted_code.splitlines()[-1])\n",
        "    print(\"modified_line\", modified_line)\n",
        "\n",
        "\n",
        "  else: #possibly only output variable listed\n",
        "\n",
        "    modified_line = \"\\n\"+\"print(\"+extracted_code.splitlines()[-1]+\")\"+\"\\n\"\n",
        "    print(\"No, it does not start with df- displaying\")\n",
        "    print(extracted_code.splitlines()[-1])\n",
        "    print(\"modified_line\", modified_line)\n",
        "\n",
        "  # remove the last df line and insert the modified line\n",
        "  extracted_code = remove_last_line_from_string(extracted_code)\n",
        "  extracted_code = extracted_code+modified_line\n",
        "\n",
        "  # add triple quote before execution\n",
        "  extracted_code = add_triple_quote(extracted_code)\n",
        "  exec(eval(extracted_code))\n",
        "\n",
        "  return extracted_code\n"
      ],
      "metadata": {
        "id": "hJCjwaiBbCmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deliver_final_answer(user_query: str, agent: object, df_response:pd.DataFrame):\n",
        "\n",
        "    response = agent({\"input\":user_query})\n",
        "\n",
        "    final_code = run_extracted_code(df_response, response)\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "0v0-S3cvcUrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. Answer it starting with '/api'.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template_okta_api = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "ptQafikwVEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template_REST_verb = \"\"\"Answer the question based on the context below. The possible answers are \"GET\", \"PUT\", \"DELETE\" or \"POST\".\n",
        "Answer one of the possible anwers\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template_rest_verb = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template_REST_verb\n",
        ")\n"
      ],
      "metadata": {
        "id": "4HNYDtfCWHAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "def obtain_okta_query(query, prompt_template_name):\n",
        "\n",
        "    openai = OpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0,\n",
        "    model_name='text-davinci-003')\n",
        "\n",
        "    return_value = openai(\n",
        "    prompt_template_name.format(\n",
        "        query= query\n",
        "    ))\n",
        "\n",
        "    print(return_value)\n",
        "    return return_value"
      ],
      "metadata": {
        "id": "R4vfxP-adi32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "okta_domain = str('trial-XXXX.okta.com')\n",
        "client_id = str(\"XXX\")\n",
        "client_secret = str(\"XXX\")"
      ],
      "metadata": {
        "id": "R6-KuZ8tfaK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "import urllib\n",
        "\n",
        "def connect_to_okta(api_call, REST_verb):\n",
        "\n",
        "    okta_domain = str('trial-XXXXX.okta.com')\n",
        "    client_id = str(\"XXXX\")\n",
        "    client_secret = str(\"XXXXX\")\n",
        "    new_secret = str('XXXX')\n",
        "\n",
        "    REST_verb = REST_verb[1:]\n",
        "    api_url = urllib.parse.urljoin(f\"https://{okta_domain}\", api_call)\n",
        "\n",
        "    auth_value = base64.b64encode(f\"{client_id}:{client_secret}\".encode(\"ascii\")).decode(\"ascii\")\n",
        "\n",
        "    response = requests.request(\n",
        "    method = REST_verb,\n",
        "    url=api_url,\n",
        "\n",
        "    headers={\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Authorization\": f\"SSWS {new_secret}\",\n",
        "        \"Content-type\": \"application/json\"\n",
        "    },\n",
        "    )\n",
        "\n",
        "    # load json into a python object\n",
        "    obj = response.json()\n",
        "\n",
        "    # flatten the json nested data\n",
        "    df_response = pd.json_normalize(obj)\n",
        "\n",
        "    #display(df)\n",
        "    return df_response"
      ],
      "metadata": {
        "id": "iHjetuZpg-WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template_context = \"\"\"Answer the question based on the context below. If the\n",
        "    question cannot be answered using the information provided answer\n",
        "    with \"I don't think so\".\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer: \"\"\"\n",
        "\n",
        "prompt_template_context = PromptTemplate(\n",
        "        input_variables=[\"context\", \"query\"],\n",
        "        template=template_context\n",
        "    )"
      ],
      "metadata": {
        "id": "EKAZtWB9MCDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_ai_response_from_context(context, query):\n",
        "\n",
        "    openai = OpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0,\n",
        "    #model_name='text-davinci-003'\n",
        "    model_name='chat-gpt4'\n",
        "        )\n",
        "\n",
        "    result_based_on_context = openai(\n",
        "        prompt_template_context.format(\n",
        "            context = context,\n",
        "            query= query\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return result_based_on_context"
      ],
      "metadata": {
        "id": "_KkAG3EV4iHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def deliver_context_sleuth(context_sleuth):\n",
        "  return context_sleuth\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_CVCfMytwiON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THE BELOW I Have two questions at the same time , but DID NOT WORK VERY WELL SO WE ARE NOT USING IT FOR NOW\n",
        "\n",
        "import langchain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "#\n",
        "#template_all = \"\"\"Answer the question first with one of the possible choices.\\\n",
        "#                  The choices are \"GET\", \"PUT\", \"DELETE\" or \"POST\".\n",
        "#                  And answer the question again starting with '/api'\n",
        "#\n",
        "#   Question: {query}\n",
        "#   Answer: \"\"\"\n",
        "#\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "prompt_template_all = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template_all\n",
        ")\n",
        "\n",
        "from langchain import OpenAI\n",
        "\n",
        "query = \"what okta api http verb and api_call to list all users\"\n",
        "\n",
        "openai = OpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0,\n",
        "    model_name='text-davinci-003'\n",
        ")\n",
        "\n",
        "print(openai(\n",
        "    prompt_template_all.format(\n",
        "        query= query\n",
        "    )\n",
        "))\n",
        "\n",
        "return_all = openai(\n",
        "    prompt_template_all.format(\n",
        "        query= query\n",
        "    )\n",
        ")\n",
        "\n",
        "print(return_all)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "oLIa92xrbJtv",
        "outputId": "b93a9920-bf1c-4194-b425-4751026bfcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nopenai = OpenAI(\\n    openai_api_key=OPENAI_API_KEY,\\n    temperature=0,\\n    model_name='text-davinci-003'\\n)\\n\\nprint(openai(\\n    prompt_template_all.format(\\n        query= query\\n    )\\n))\\n\\nreturn_all = openai(\\n    prompt_template_all.format(\\n        query= query\\n    )\\n)\\n\\nprint(return_all)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}